# Bioinformatic Protocols for NLHPC
### Documentaci贸n con Tips y Buenas Practicas para el uso del NLHPC para tareas de Bioinform谩tica y/o similares<br>

El servicio del Laboratorio Nacional de Computaci贸n de Alto Rendimiento o [NLHPC](url:https://www.nlhpc.cl/) es un supercomputador a gran escala dise帽ado para dar respuesta a los desaf铆os cientificos y tecnol贸gicos de nuestro pais.
Esta infraestructura est谩 al servicio de la comunidad y puede ser utilizada en 谩reas como Astronom铆a, Ingenier铆a, Salud y Bioinform谩tica, siendo esta 煤ltima la que exploraremos en el siguiente documento.

Durante el transcurso de su servicio, el NLHPC a podido ayudar al desarrollo de la ciencia de datos del genoma humano, dar sequimiento a enfermedades como el coronavirus, y acelerado descubrimientos importantes tanto en desiertos, oceanos y medio ambiente.
Ejemplos notables de este apoyo a la ciencia se pueden observar en publicaciones tales como:
<br>
*   [Genomes of the Orestias pupfish from the Andean Altiplano shed light on their evolutionary history and phylogenetic relationships within Cyprinodontiformes](https://doi.org/10.1186/s12864-024-10416-w)
*   [From sewage to genomes: Expanding our understanding of the urban and semi-urban wastewater RNA virome](https://doi.org/10.1016/j.envres.2025.121509)
*    [Fine Mapping Using Whole-Genome Sequencing Confirms Anti-M眉llerian Hormone as a Major Gene for Sex Determination in Farmed Nile Tilapia (Oreochromis niloticus L.)](https://doi.org/10.1534/g3.119.400297)
*   [Insights into neutralizing antibody responses in individuals exposed to SARS-CoV-2 in Chile](https://doi.org/10.1126/sciadv.abe6855)
*   [Nutrient Scarcity in a New Defined Medium Reveals Metabolic Resistance to Antibiotics in the Fish Pathogen Piscirickettsia salmonis](https://doi.org/10.3389/fmicb.2021.734239))
<br>

No obstante, el acceso y uso del NLHPC se puede percibir como una barrera inpenetrable para nuevos usuarios, los cuales tienen que batallar no solo con el entorno UNIX, sino tambi茅n, con el uso espec铆fico de estas tecnolog铆as que distan mucho del manejo diario de nuestros computadores personales.

Para poder enfrentar este desaf铆o, se ha preparado esta gu铆a para orientar tanto a nuevos investigadores como usuarios antiguos, la cual permitira utilizar de manera sencilla y eficiente las instalaciones del NLHPC.



## Tabla de Contenidos
* [1. Creaci贸n de cuentas, solicitudes de horas de computo, CPUs y Storage.](#1-creaci贸n-de-cuentas-solicitudes-de-horas-de-computo-cpus-y-storage)
    * [1.1 Creaci贸n de cuentas](#11-creaci贸n-de-cuentas)
    * [1.2 Solicitudes de horas de computo.](#12-solicitudes-de-horas-de-computo)
    * [1.3 CPUs y Storage](#13-cpus-y-storage)
* [2. Login al NLHPC.](#2-login-al-nlhpc)
* [3. Instalaci贸n de software mediante gestor de paquetes, Modo SUDO y Module LOAD](#3-instalaci贸n-de-software-mediante-gestor-de-paquetes-modo-sudo-y-module-load)
    * [3.1 Instalaci贸n de software mediante gestor de paquetes](#31-instalaci贸n-de-software-mediante-gestor-de-paquetes)   
    * [3.2 Modo SUDO](#32-modo-sudo)
       * [3.2.1 Instalaci贸n local con el codigo fuente.](#321-instalacion-local-con-el-codigo-fuente)
       * [3.2.2 Solicitud de instalaci贸n al equipo de Soporte NLHPC](#322-solicitud-de-instalacion-al-equipo-de-soporte-nlhpc)
    * [3.3 Module Load](#33-module-load)
* [4. Archivo de ejecuci贸n de tareas / Jobs en la cola de procesos y solicitud de recursos](#4-archivo-de-ejecuci贸n-de-tareas--jobs-en-la-cola-de-procesos-y-solicitud-de-recursos)
    * [4.1 Archivo de ejecuci贸n de tareas / Jobs](#41-archivo-de-ejecuci贸n-de-tareas--jobs)
    * [4.2 Solicitud de Recursos](#42-solicitud-de-recursos)
* [5. Ejecuci贸n de tareas / jobs, Monitoreo de tareas y Cancelaci贸n de tareas](#5-ejecuci贸n-de-tareas--jobs-monitoreo-de-tareas-y-cancelaci贸n-de-tareas)
    * [5.1 Ejecuci贸n de tareas](#51-ejecuci贸n-de-tareas)
        * [5.1.1 Ejecuci贸n lineal](#511-ejecuci贸n-lineal)
        * [5.1.2 Ejecuci贸n en paralelo individual](#512-ejecuci贸n-en-paralelo-individual)
        * [5.1.3 Ejecuci贸n en paralelo por grupo](#513-ejecuci贸n-en-paralelo-por-grupo)
    * [5.2 Monitoreo de tareas](#52-monitoreo-de-tareas)
    * [5.3 Cancelaci贸n de tareas](#53-cancelaci贸n-de-tareas)
* [6. Errores de ejecuci贸n de tareas / jobs.](#6-errores-de-ejecuci贸n-de-tareas--jobs)
    * [6.1 error JOB XXXXXXXXX ON mn00P CANCELLED AT (DATE)](#61-error-job-xxxxxxxxx-on-mn00p-cancelled-at-date)
    * [6.2 OOM_KILL EVENT. Some of the step tasks have been OOM Killed.](#62-oom_kill-event-some-of-the-step-tasks-have-been-oom-killed)
    * [6.3 Segmentation fault (core dumped). Aborted (core dumped)](#63-segmentation-fault-core-dumped-aborted-core-dumped)
* [7. Agradecimientos al NLHPC en publicaciones](#7-agradecimientos-al-nlhpc-en-publicaciones)

## 1. Creaci贸n de cuentas, solicitudes de horas de computo, CPUs y Storage.

### 1.1 Creaci贸n de cuentas

La creaci贸n de cuentas del NLHPC se puede clasificar en 2 grupos: 1) Usuarios nuevos sin conexi贸n previa al NLHPC y 2) Usuarios que ser谩n parte de un grupo ya existente en el NLHPC.

1. En el primer grupo de usuarios, la creaci贸n de cuentas se da de forma exclusiva a un/una PI
    (Investigador Principal) o investigadores j贸venes, quienes deber谩n rellenar el siguiente
    formulario con la informaci贸n correspondiente: https://solicitudes.nlhpc.cl/.

2. En el caso de que el PI est茅 generando una cuenta para su grupo o equipo y requiera m谩s de una
    cuenta, lo ideal es generar 1 cuenta de PI en https://solicitudes.nlhpc.cl/ y cuentas individuale
    s para cada miembro del grupo, es decir:

    a) 1 Cuenta de PI. <br>
    b) N Cuentas, 1 por cada miembro del Equipo (Team).

La recomendaci贸n principal al crear cuentas para un grupo es, idealmente, asignar un Nombre del Equipo de Investigaci贸n. Como ejemplos: CRG-LAB1, CRG_UOH o, en mi caso particular, Mathomics. Esto facilitar谩 la posterior creaci贸n de cuentas para miembros del mismo grupo y la asignaci贸n de permisos, protocolos y herramientas (tools) que el grupo completo necesitar谩 eventualmente.

La recomendaci贸n principal es que cada nueva cuenta sea creada o solicitada por un solo miembro de cada Equipo o Grupo. Esta tarea puede recaer directamente en el PI o en un encargado seleccionado.

Para mantener el orden y la trazabilidad de los datos, lo ideal es que cada usuario pueda acceder a una sola cuenta. Compartir claves conlleva una alta probabilidad de accesos no autorizados, uso de recursos no controlados o eliminaci贸n de datos sin consentimiento. En el caso de necesitar una cuenta de datos que ser谩 utilizada solo por periodos cortos de tiempo por varios usuarios distintos, se recomienda crear una cuenta especial de invitado. Esta cuenta debe estar registrada con el mismo PI y bajo el mismo Equipo. Por lo general, estas cuentas se usan solo para compartir datos con otros investigadores o estudiantes en pr谩ctica que no necesitan cuentas permanentes individuales.

### 1.2 Solicitudes de horas de computo.

Durante la creaci贸n de la cuenta del PI, se requerir谩 establecer una cuota de horas de c贸mputo. Esta solicitud se asignar谩 como l铆mite de ejecuci贸n de tareas con una duraci贸n de 1 a帽o, a partir de la fecha de la primera solicitud. Estas horas de c贸mputo se establecer谩n como l铆mite para cada Equipo, por lo que no es necesario generar una petici贸n de horas de c贸mputo por cada cuenta.

En el caso de que las horas de c贸mputo se cumplan, ninguna cuenta del Equipo podr谩 seguir ejecutando tareas hasta generar una nueva solicitud en https://solicitudes.nlhpc.cl/. Por lo general, una solicitud de horas de c贸mputo deber谩 tomar en cuenta la cantidad de miembros, las tareas que se necesitan ejecutar y la cantidad de veces que se necesita ejecutar la misma tarea. Como un buen estimador en bioinform谩tica, la cantidad de horas para limpiar, ensamblar, anotar y analizar genomas durante 1 a帽o calendario es de entre 20.000 y 50.000 horas por cuenta.

En el caso de cuentas con gran volumen de datos y varios an谩lisis al a帽o, las horas por cuenta pueden superar f谩cilmente las 250.000 horas de c贸mputo. La recomendaci贸n es:

    * Para Equipos nuevos: entre 10.000 a 15.000 horas de c贸mputo por cuenta.

    * Para Equipos con experiencia: entre 50.000 a 70.000 horas de c贸mputo por cuenta.

En el caso de que la cantidad de horas de c贸mputo solicitadas sea muy por debajo de lo necesario por cada Equipo, el equipo de Soporte del NLHPC enviar谩 un correo a cada cuenta con la notificaci贸n de que el n煤mero de horas est谩 por acabarse o completamente utilizado. En esta notificaci贸n, cada cuenta tendr谩 desglosado el n煤mero de horas que se utilizaron.

Para no repetir una nueva solicitud de horas de c贸mputo cada 4 o 6 meses, lo ideal es utilizar la informaci贸n obtenida en la notificaci贸n del NLHPC y reajustar la petici贸n para que coincida con el periodo de 12 meses.

#### El cobro de horas de c贸mputo se realiza sobre el uso real, y no sobre las horas solicitadas en la creaci贸n de cuentas. #### <br>
### 1.3 CPUs y Storage

Al crear una nueva cuenta en el NLHPC, si no se especifica la cantidad de recursos de CPU y Almacenamiento, cada cuenta tendr谩 por defecto 88 CPU y 120 GB de Almacenamiento (Storage).
En el caso de las CPU, esto significa que el usuario puede utilizar un m谩ximo de 88 CPU al mismo tiempo durante procesos de ejecuci贸n. Es decir, si ejecutamos 3 procesos con 40 CPU cada uno:

      a) Proceso 1 con 40 CPU. Este proceso se ejecutar谩 sin problemas, siempre y cuando existan recursos en el NLHPC. 
      b) Proceso 2 con 40 CPU. Este proceso se ejecutar谩 sin problemas, siempre y cuando existan recursos en el NLHPC.
      c) Proceso 3 con 40 CPU. Este proceso no se podr谩 ejecutar hasta que el proceso a) o b) terminen. 

En el caso del Storage, este valor es el limite maximo con el cual podremos guardar nuestros archivos o tareas. Si se da la instancia de que nuestra cuota de Storage esta en su limite, no se podran seguir ejecutando tareas, aun teniendo CPUs y horas de computo disponible. 
En el caso de que lleguemos al limite del Storage, durante la ejecuci贸n de una de nuestras tareas, esta terminara antes forzosamente, y se notificara al usuario de la falta de espacio en nuestra cuenta.

#### Tanto la cantidad de CPU como el Almacenamiento se pueden solicitar de forma individual para cada cuenta/usuario, por lo que es necesario revisar los requerimientos de cada uno. ####

A modo general el uso de recursos optimo es alrededor de:

    100 CPUs y 500Gb para Team nuevos. 
    200+ CPUs y 1Tb+ de Storage para Teams antiguos 

<br>
<br>

## 2. Login al NLHPC.
<br>
Una vez creada nuestra cuenta, podemos acceder al Login del NLHPC. El proceso actual, con fecha 2025, es el siguiente:

```
ssh -p 4603 mi_cuenta@leftraru.nlhpc.cl
```

Con este comando podremos entrar al NLHPC en cualquier parte del pa铆s, sin necesidad de una VPN.
En el caso de encontrarnos temporalmente en el extranjero, se deber谩 utilizar una VPN para anclarnos primero a Chile y luego acceder al servidor de manera normal.

<br>
<br>

## 3. Instalaci贸n de software mediante gestor de paquetes, Modo SUDO y Module LOAD.<br>

### 3.1 Instalaci贸n de software mediante gestor de paquetes
La mayor铆a de las herramientas (tools) y paquetes en Bioinform谩tica se pueden encontrar en gestores de paquetes altamente utilizados en ciencia, tales como:
```
Anaconda3
Miniconda
Mamba
HomeBrew
PERL
PIP
```
Tanto Anaconda3 como Miniconda funcionan sin problemas en el NLHPC (https://www.anaconda.com/docs/getting-started/miniconda/install#linux-x86) y se pueden instalar de manera local en nuestra cuenta as铆:

```
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ~/Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
```
Una vez instalados, es tan simple como instalar cualquier herramienta que exista en el gestor de paquetes. En nuestro caso, usaremos como ejemplo **Meme Suite**. (https://anaconda.org/bioconda/meme):

```
conda install bioconda::meme
conda install bioconda/label/cf201901::meme
```

Eventualemte, la instalaci贸n se ejecutara sin problemas y podremos utilizar la herrameinta como corresponda.<br>

### 3.2 Modo SUDO

En el caso de nuestros computadores personales, podremos acceder a la instalaci贸n de tareas mediante SUDO o superusuario.
Este m茅todo de instalaci贸n est谩 restringido para todas las cuentas del NLHPC a excepci贸n del equipo de soporte, por lo que ninguna herramienta se podr谩 instalar mediante este m茅todo; es decir, los comandos tales como:

```
sudo install tool
sudo apt install tool
```
NO son validos.<br>
En el caso de necesitar instalar una herramienta que requiera espec铆ficamente este comando, existen 2 soluciones espec铆ficas:.<br>

#### 3.2.1 Instalaci贸n local con el codigo fuente. ####

Para la instalaci贸n local de nuestra herramienta, primero necesitaremos el c贸digo fuente de la misma. Por lo general, cualquier herramienta que utilice el comando SUDO tiene un c贸digo fuente, el cual se puede descargar desde la p谩gina web del software o desde GitHub. En nuestro caso, seguiremos el ejemplo de MEME Suite (https://web.mit.edu/meme/current/share/doc/install.html):

```
tar zxf meme_4.11.4.tar.gz
cd meme_4.11.4
./configure --prefix=$HOME/meme --with-url=http://meme-suite.org --enable-build-libxml2 --enable-build-libxslt
make
make test
make instal
```

En el ejemplo anterior, podemos instalar localmente nuestra herramienta utilizando el comando espec铆fico:
```
--prefix=$HOME/my_carpeta_personal
o
--prefix=/home/my_cuenta/my_carpeta_personal
```

En el caso de que no exista la opci贸n de instalaci贸n del c贸digo fuente, o encontremos errores dentro del proceso, podemos utilizar la opci贸n B.<br>
#### 3.2.2 Solicitud de instalaci贸n al equipo de Soporte NLHPC ####

Cada vez que necesitemos instalar herramientas que parecen demasiado complicadas o requieren librer铆as que no podemos encontrar, lo mejor es realizar una solicitud al equipo de Soporte del NLHPC (soporte@nlhpc.cl) mediante un correo.
```
at@ soporte@nlhpc.cl
Asunto: Instalacion tool Meme Suite:
_______________________________________

Correo.....

```
En nuestro correo de solicitud debemos agregar:

> El *link de la tool* a instalar. <br>
> El o los *usuarios* a los que se desea instalar la tool.

En el caso de que nuestra tool sea requerida por todo nuestro Team, existe la posibilidad de implementar la herramienta como Modulo, y que quede en el registro de tools instaladas del NLHPC para que pueda ser cargada por cualquier usuario, inclusive fuera de nuestro Team.
<br>

### 3.3 Module Load 

El NLHPC tiene pre-instaladas una variedad de tools las cuales han sido testeadas y deberian ejecutarse sin problemas en cualquier cuenta del servidor.

Para acceder al catalogo de herramientas pre-instaladas, podemos presionar en la consola:

```
$ module load [TAB]
Display all 1113 possibilities? (y or n)
amber                                         MCTDH
Amber                                         MCTDH/86.3
amber/20-mpi-zen4-w                           mercurial
Amber/22-fosscuda                             mercurial/6.7.3-zen4-g
Amber/22.intel-2019b                          merra2wps
Amber/24-foss-2023b                           merra2wps/08-2021
amber/24-rocm-gcc-14.2.0-zen4                 MESH
amdblis                                       MESH/1.4
........
........
........

```

Este comando listara todas las tools existentes, en el caso de buscar una nombre especifico, podemos poner las inciales o el nombre completo.

```
$ module load M[TAB]
M4                                       MCR/R2018a                               Mono
M4/1.4.17                                MCR/R2020b                               Mono/6.12.0.122
M4/1.4.18                                MCTDH                                    Mothur/intel2022.00
M4/1.4.19                                MCTDH/86.3                               Mothur/intel2022.00/1.48.1
Magics                                   MEGAHIT                                  MPC
Magics/4.13.0                            MEGAHIT/1.2.9                            MPC/1.0.1
Mako                                     Mesa                                     MPC/1.2.1
Mako/1.1.4                               Mesa/21.1.7                              MPFR
MALT                                     MESH                                     MPFR/2.4.2
MALT/0.5.2                               MESH/1.4                                 MPFR/3.1.2
Mathematica                              MESH/1.4.2-symbols                       MPFR/4.1.0
Mathematica/13.3                         Meson                                    MPPCrystal17
MathGL                                   Meson/0.58.2                             MPPCrystal17/1.0.2
MathGL/8.0.1                             MetaBAT                                  MuJoCo
MathWorksServiceHost                     MetaBAT/2.15                             MuJoCo/2.1.1
MathWorksServiceHost/2024.13.0.2         METIS                                    MultiNest
Matlab                                   METIS/5.1.0                              MultiNest/3.10
MATLAB                                   Miniconda2                               MultiQC
Matlab/2017                              Miniconda2/4.7.10                        MultiQC/1.14
MATLAB/2019b                             Miniconda3                               MUMmer
Matlab-Runtime                           Miniconda3/4.5.12                        MUMmer/4.0.0beta2
Matlab-Runtime/amd                       Miniconda3/4.9.2                         MUMmer/4.0.0rc1
Matlab-Runtime/intel                     MinPath                                  MUMPS
Maven                                    MinPath/1.6                              MUMPS/5.4.0-metis
Maven/3.6.3                              Molekel                                  MUMPS/5.5.1-metis
MCR                                      Molekel/5.4.0-Linux_x86_64               MUMPS/5.7.3-metis
MCR/R2016b                               Molpro
MCR/R2017a                               Molpro/mpp-2020.2.1.linux_x86_64_openmp
```

El comando module load o ml para abreviar, es sencible a mayusculas y minisculas:
```
$ module load M[TAB]
$ module load m[TAB]
$ module load ME[TAB]
$ module load Me[TAB]
$ module load me[TAB]
```

Cada uno de los comandos anteriores generara resultados diferentes, por lo que es necesario tomar precauciones a la hora de cargar un modulo.

Ademas, existen diferentes versiones para las tools pre-instaladas, por lo que habra que poner atenci贸n a la hora de cargar la tool y version correctas.

```
$ module load Java
Java                        Java/17.0.2                 Java/1.8.0_202
Java/11                     Java/17.0.4                 Java/1.8.0_232-b09-OpenJDK
Java/11.0.2                 Java/1.8                    Java/23.0.2
```
<br>
<br>

## 4. Archivo de ejecuci贸n de tareas / Jobs en la cola de procesos y solicitud de recursos .<br>

### 4.1 Archivo de ejecuci贸n de tareas / Jobs <br>

Todas las tareas que necesitemos ejecutar en el NLHPC deben entrar en la cola de procesos a trav茅s de un archivo de ejecuci贸n.
Ejecutar tareas directamente en la consola, sin pasar por la cola de procesos, tiene un tiempo l铆mite m谩ximo de ejecuci贸n de 30 minutos.
Lo ideal es siempre utilizar la cola de procesos, puesto que correr directamente desde la consola utiliza los recursos del sistema de login, lo cual ralentiza toda la navegaci贸n en el sistema NLHPC.

Tareas peque帽as como parsear archivos, uso de grep o awk se pueden realizar perfectamente en la consola de login. Tareas como ensambles, anotaciones u otras que requieran m谩s de una CPU o bastante memoria RAM est谩n prohibidas.

En el caso que requiramos ejecutar una tarea en la cola de procesos, necesitaremos un archivo de ejecuci贸n. A continuaci贸n, tendremos uno b谩sico que nos servir谩 para la mayor铆a de nuestras tareas.

1) Primero, debemos crear un archivo con un editor de texto, para ello usaremos VIM.

```
$ vim standar_job.sh
```

2) Luego dentro del archivo podremos escribir la siguiente configuraci贸n:
   
```
#SBATCH -J [JOB_NAME]
#SBATCH -p [PARTITION]
#SBATCH -n [NUMBER OF NODES]
#SBATCH -c [NUMBER OF CPU]
#SBATCH --mem=[RAM MEMORY IN GB]
#SBATCH -o [OUTPUT FILE] 
#SBATCH -e [ERROR FILE]
#SBATCH -t [EXECUTION TIME]
#SBATCH --mail-user=your-email-here@examp.com
#SBATCH --mail-type=ALL

Aca-nuestro-programa

```
<br>

### Revisemos ahora cada uno de los parametros: ####

<br>


```
#SBATCH -J [JOB_NAME] :           Aqu铆 ir谩 el nombre de nuestra tarea. Utilizar nombres cortos y f谩ciles de identificar
                                  es lo recomendado.
```

```
#SBATCH -p [PARTITION] :          En este campo ir谩 la partici贸n del NLHPC a la cual enviaremos nuestra tarea. Actualmente,
                                  existen 6 particiones que los usuarios pueden utilizar. Las caracter铆sticas de cada
                                  partici贸n se encuentran en https://dashboard.nlhpc.cl/.
                                  Estas particiones son:
                                  main,
                                  general,
                                  largemem,
                                  mi210,
                                  mi100,
                                  v100.
                                  Se deber谩 escoger la partici贸n seg煤n las necesidades que tengamos para ejecutar
                                  nuestras herramientas. La mayor parte de nuestras tareas correr谩n en "general",
                                  "main" o "largemem", puesto que CPU y RAM componen la mayor parte de las herramientas
                                  en Bioinform谩tica.

```


```
#SBATCH -n [NUMBER OF NODES]:     Este par谩metro indica el n煤mero de NODOS para ejecutar nuestras tareas.
                                  En Bioinform谩tica, muy pocas herramientas est谩n optimizadas para ejecutarse en m谩s
                                  de 1 NODO, por lo que es pr谩cticamente improbable utilizar m谩s de 1
```



```
#SBATCH -c [NUMBER OF CPU] :      Aqu铆 se establece el n煤mero de CPU a utilizar. Por m谩s que podamos pedir 100 CPU, la mayor
                                  parte de las herramientas en Bioinform谩tica solo escalan hasta 12 o 20 CPU, con algunas
                                  excepciones llegando a los 40 CPU. Pedir m谩s no nos dar谩 una ventaja real de tiempo de
                                  ejecuci贸n. Al contrario, perderemos m谩s tiempo esperando que se liberen los recursos que
                                  ejecutar las tareas con la mitad de CPU. Formato: N煤meros enteros del 1 al 256, dependiendo
                                  de cada partici贸n:
                                  L铆mite CPU partici贸n main = 256;
                                  L铆mite CPU partici贸n general = 44;
                                  L铆mite CPU partici贸n largemem = 44.
```


```
#SBATCH --mem= [RAM MEMORY IN GB] :Este par谩metro reserva la cantidad de memoria RAM que necesitar谩 nuestra herramienta.
                                  La cantidad de RAM se debe establecer en relaci贸n al NODO que se est谩 reservando;
                                  es decir, no es posible reservar 200 GB de RAM en la partici贸n "general" ni 1 TB de RAM
                                  en la partici贸n "largemem". El equipo del NLHPC recomienda utilizar a lo m谩ximo 5 GB de
                                  RAM por CPU, aunque esta recomendaci贸n es casi inutilizable en Bioinform谩tica.
                                  En nuestro caso, existir谩n herramientas que utilizar谩n 200 a 400 GB de RAM, pero se
                                  ejecutar谩n solo en 20 a 40 CPU.
                                  M谩s adelante veremos algunos ejemplos de ejecuci贸n. Formato: N煤meros enteros del 1 al 710,
                                  seguido del sufijo GB, dependiendo de cada partici贸n:
                                  L铆mite RAM partici贸n main = 710 GB;
                                  L铆mite RAM partici贸n general = 175 GB;
                                  L铆mite RAM partici贸n largemem = 710 GB

   
```

```
#SBATCH -o [OUTPUT FILE] :        Este archivo contiene la salida por consola de nuestra herramienta. Cuando la
                                  herramienta que estamos tratando de ejecutar no tiene configurado un archivo de output,
                                  la salida de nuestro programa ser谩 escrita aqu铆. La mayor parte de los archivos output
                                  ser谩 un mensaje propio de cada herramienta con informaci贸n y tiempo de ejecuci贸n.
```

```
#SBATCH -e [ERROR FILE] :         Este archivo contendr谩 la salida de errores de la ejecuci贸n de nuestra herramienta.
                                  Aqu铆 podremos revisar si nuestra ejecuci贸n fue exitosa o si existi贸 alg煤n problema
                                  particular que produjo la terminaci贸n forzosa de nuestro job. Es importante siempre
                                  configurar este archivo, pues es la 煤nica forma real de reconocer errores para futuras
                                  ejecuciones. (Nota: Intercambi茅 la descripci贸n de -o y -e ya que -e suele ser
                                  Error est谩ndar).
```

```
#SBATCH -t [EXECUTION TIME] :     Este par谩metro ha sido recientemente introducido en el a帽o 2025 y reserva el tiempo de
                                  ejecuci贸n de nuestra tarea. No es necesario ser extremadamente exacto en los tiempos de
                                  ejecuci贸n, pero generalmente podemos utilizar horas o d铆as sin mayores cambios de esta
                                  variable. Si nuestra tarea supera el tiempo reservado, esta se cancelar谩 autom谩ticamente
                                  y tendremos que volver a lanzarla desde el inicio. Formato: dd-hh:mm:ss.
                                  Ejemplo para 1 d铆a, 3 horas y 20 minutos de ejecuci贸n ser铆a: 1-03:20:00.
```
```
#SBATCH --mail-user=your-email-here@examp.com : Opcional. Generar谩 un correo autom谩tico con informaci贸n de nuestra tarea.  
#SBATCH --mail-type=ALL                       :  Opcional. Indica que env铆e correo en todos los estados (BEGIN, END,
                                                 FAIL, etc.).
                                                
```

<br>


Ahora que ya sabemos como configurar nuestro archivo de ejecuci贸n, veamos un ejemplo real.
   
```
#SBATCH -J GBK
#SBATCH -p general
#SBATCH -n 1
#SBATCH -c 1
#SBATCH --mem=6GB
#SBATCH -o error.%a.%N.%j.out 
#SBATCH -e error.%a.%N.%j.err
#SBATCH -t 06:00:00
#SBATCH --mail-user=your-email-here@examp.com
#SBATCH --mail-type=ALL

python gbk_generator.py my_input_1 my_input_2 > salida_programa.tsv

```

<br>



### 4.2 Solicitud de Recursos ###

Una vez revisado cada campo de nuestro Archivo de ejecuci贸n de tareas, podremos ahora solicitar la cantidad de recursos y el tiempo que estimemos conveniente. 
Como se mencion贸 brevemente, una herramienta que pueda utilizar m谩s de 1 CPU no necesariamente se ejecutar谩 70 veces m谩s r谩pido al pedir 70 CPU.

La experiencia nos dice que la mayor cantidad de herramientas grandes escala solo hasta los 20 a 40 CPU. Adem谩s, es mucho m谩s eficiente dividir nuestras tareas en fragmentos o chunks y ejecutarlos al mismo tiempo, que ejecutar todo junto pidiendo m谩s CPU;
es decir, ejecutar la misma tarea, dividida en 5 partes con 20 CPU cada una, ser谩 mucho m谩s eficiente que ejecutar la misma tarea completa 1 vez con 100 CPU.

De todas formas, existe un cat谩logo infinito de herramientas que podremos utilizar, y aunque no podemos conocer a priori sus requerimientos de ejecuci贸n de m谩xima eficiencia, la experiencia nos brindar谩 un mejor juicio a la hora de escoger recursos.
<br>
Pero a falta de tiempo, he aqu铆 algunos ejemplos:

### TAREAS DE ASSEMBLY. 1 Metegenoma grande de 200+ Millones de Reads. Tool: MEGAHIT ###

```
#SBATCH -J Assembly
#SBATCH -p main o largemem
#SBATCH -n 1
#SBATCH -c [12-20]
#SBATCH --mem=[200GB-400GB]
#SBATCH -t 2-00:00:00
```

### TAREAS DE ANOTACION.  Mismo metagenoma, contra una base de datos de 50GB. Tool: EggNOGMapper ###

```
#SBATCH -J annotation
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c [8 a 12]
#SBATCH --mem=80GB
#SBATCH -t 4-00:00:00
```

### CLUSTERING DE GENES.  400+ Millones de CDS. Tool: MMseqs2 ###

```
#SBATCH -J cluster
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c [12 a 20]
#SBATCH --mem=[400GB - 500GB]
#SBATCH -t 3-00:00:00
```

### CREACION DE MAGS / BINNING. Tool: SemiBin2 ### 

```
#SBATCH -J binning
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c [12 a 20]
#SBATCH --mem=[20GB - 150GB]
#SBATCH -t [2-00:00:00 a 7-00:00:00]
```

### ANOTACION DE MAGS. 1 MAG. Tool: Bakta: ###

```
#SBATCH -J mag_annot
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c 2
#SBATCH --mem=20GB
#SBATCH -t 08:00:00
```

### ANOTACION DE MAGS. 1000+ MAG. Tool: DRAM: ###

```
#SBATCH -J mag_annot
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c [12 a 20]
#SBATCH --mem=200GB
#SBATCH -t 14-23:00:00
```

### PARSEO DE BASES DE DATOS / SCRIPTS DE PYTHON, ENTRE OTROS: ###

```
#SBATCH -J job
#SBATCH -p general o main
#SBATCH -n 1
#SBATCH -c 1
#SBATCH --mem=[5GB - 40GB]
#SBATCH -t [06:00:00 a 1-00:00:00]
```


Estos son algunos ejemplos de las tareas con mayor cantidad de recursos que he tenido experiencia de probar en el NLHPC. Cabe considerar que estos datos tienen un peso aproximado de TBs, 
por lo que proyectos relativamente livianos requerir谩n menos recursos. Cabe destacar que la regla de 1 CPU por cada 5 GB de RAM no aplica para ninguno de los programas listados.
Esta regla es m谩s bien una recomendaci贸n de eficiencia en programas altamente revisados y, lamentablemente, no aplica para la mayor铆a de herramientas bioinform谩ticas. 
Mientras utilicemos rangos de CPU y RAM que no se salgan de extremos, como por ejemplo, ejecutar una herramienta single thread con 50 CPU y 700 GB de RAM, no deber铆amos tener mayores problemas.

###########################################################################################################
## IMPORTANTE ##
#### Todas las tareas / jobs, tienen un maximo tiempo de ejecucion de 30 DIAS. Luego de esto, sin importar el tiempo pedido, la tarea sera cancelada automaticamente ####

###########################################################################################################

驴Y qu茅 hacemos si nuestra herramienta necesita correr por m谩s de 30 d铆as y no queremos que se cancele autom谩ticamente?

Para estas excepciones y otras, podemos enviar un correo a soporte@nlhpc.cl y enviar nuestro *JOBID* de la tarea en ejecuci贸n para pedir una extensi贸n de tiempo.

<br>
<br>

## 5. Ejecuci贸n de tareas / jobs, Monitoreo de tareas y Cancelaci贸n de tareas <br>

### 5.1 Ejecuci贸n de tareas


Una vez que ya tenemos creado nuestro archivo de ejecuci贸n de tareas y alocado nuestros recursos, debemos ahora lista y ejecutar nuestras tareas.

Para ello, tomaremos nuevamente nuestro archivo de ejemplo.


```
#SBATCH -J python_script
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c 1
#SBATCH --mem=[10GB - 50GB]
#SBATCH -t 1-00:00:00

python my_tool.py my_input_file.tsv > output_one.tsv

python my_second_tool.py output_one.tsv > output_two.tsv

echo "Task Completed"


```
En este caso, nuestro archivo de ejecuci贸n tratara de ejecutar un script de python llamado my_tool.py y este a su vez requerira un archivo de entrada llamado my_input_file.tsv. Este script generara un arvhivo de salida output_one.tsv que sera utilizado ahora como input por nuestro segundo script my_second_tool.py y generara la salida final output_two.tsv.

Como se puede observar, se pueden listar mas de una ejecuci贸n o linea de comando en nuestro archivo de ejecuci贸n de tareas, done incluso podremos construir pipelines completos para tareas especificas.

Una vez que tengamos todos los comandos a ejecutar, guardaremos nuestro archivo de ejecuci贸n, en este caso, nuetro ejemplo se llamar谩, standar.sh

```
standar.sh

```

Para poder ejecutar ahora nuestros comandos dentro del archivo standar.sh, lo que debemos hacer es llamar en la consola a la funci贸n sbatch con el nombre de nuestro archivo de ejecuci贸n y luego presionar enter.

```
$ sbatch standar.sh
```

Si nuetro archivo de ejecucio贸n esta correcto, nos aparecera el mensaje:

```
Submitted batch job XXXXXXXXXXXXXXX
```

Este numero de job es importante, puesto que nos permitira rastrear el status de nuestra tarea a futuro.

Pero, 驴Que pasa si quiero ejecutar mas de 1 tarea al mismo tiempo?

En este caso, tenemos varias opciones. 

#### 5.1.1 Ejecuci贸n lineal

Para este tipo de ejecuci贸n, solo debemos escribir todos nuestros comandos juntos en el mismo archivo de ejecuci贸n de tareas, cada ejecuci贸n debajo de la siguiente, es decir:

```
#SBATCH -J python_script
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c 1
#SBATCH --mem=[10GB - 50GB]
#SBATCH -t 4-00:00:00

#Primera ejecuci贸n

python my_tool.py my_input_file_1.tsv > output_one_1.tsv
python my_second_tool.py output_one_1.tsv > output_two_1.tsv

# Segunda ejecuci贸n

python my_tool.py my_input_file_2.tsv > output_one_2.tsv
python my_second_tool.py output_one_2.tsv > output_two_2.tsv

# Tercera ejecuci贸n

python my_tool.py my_input_file_3.tsv > output_one_3.tsv
python my_second_tool.py output_one_3.tsv > output_two_3.tsv

# Cuarta ejecuci贸m

python my_tool.py my_input_file_4.tsv > output_one_4.tsv
python my_second_tool.py output_one_4.tsv > output_two_4.tsv

echo "Task Completed"

```

Con este tipo de archivo de ejecuci贸n, todas las tareas se ejecutaran de manera linea, y cada comando debera esperar al anterior para ser ejecutado. Si bien es cierto, esta el la mejor forma para generar pipelines, no es muy eficiente si queremos ejecutar la misma tarea muchas veces. 

###########################################################################################################
## IMPORTANTE ##
#### 
Hay que recordar que si 1 sola tarea toma 1 dia en ejecutarse, ahora tendremos que solicitar 4 veces mas tiempo de ejecuci贸n. ####

###########################################################################################################

Pero que pasa si nuestros tiempo de ejecuci贸n son demasiado largos? es necesario esperar 7 dias por 7 ejecuciones?. Ahora veremos soluciones a este problema.

#### 5.1.2 Ejecuci贸n en paralelo individual

En esta ocasi贸n modificaremos nuestro archivo de ejecuci贸n de tareas para poder hacer uso del traspaso de nombres de variables por consola, y asi, generalizar nuestros scripts.

```
#SBATCH -J python_script
#SBATCH -p main
#SBATCH -n 1
#SBATCH -c 1
#SBATCH --mem=[10GB - 50GB]
#SBATCH -t 1-00:00:00

#Ejecuci贸n estandar

python my_tool.py $1.tsv > $2.tsv
python my_second_tool.py $2.tsv > $3.tsv
```

En esta versi贸n del script, hemos reemplazado los nombres originales, donde ahora, hemos generalizado los nombres my_input_file_N.tsv a $1, asi tambien como los archivos output_one_N.tsv por $2 y output_two_N.tsv por $3.

Con ello ahora nuestra llamada al archivo de ejecuci贸n de tareas cambiara de la siguiente forma:

```
# Antiguo metodo de ejecuci贸n
$ sbatch standar.sh

# Nuevo metodo de ejecuci贸n
$4 sbatch standar.sh my_input_file_N.tsv output_one_N.tsv output_two_N.tsv
```

Es aqu铆 donde podemos observar la mayor ventaja del m茅todo paralelo. El unico limite de numero de tareas a llamar sera la cantidad de CPU que nuestra cuenta tenga acceso, por lo que ahora, nuestras 4 llamadas de scripts anteriores cambiar谩 a:

```
$ sbatch standar.sh my_input_file_1.tsv output_one_1.tsv output_two_1.tsv
Submitted batch job 000001
$ sbatch standar.sh my_input_file_2.tsv output_one_2.tsv output_two_2.tsv
Submitted batch job 000002
$ sbatch standar.sh my_input_file_3.tsv output_one_3.tsv output_two_3.tsv
Submitted batch job 000003
$ sbatch standar.sh my_input_file_4.tsv output_one_4.tsv output_two_4.tsv
Submitted batch job 000003
```
Este metodo asegurara ahora que podamos ejecutar nuestras 4 tareas al mismo tiempo, por lo que el tiempo de ejecuci贸n pasara de 4 dias, a 1 solo dia en el mejor de los casos (en el caso de que tengamos los recursos y sean asignados a nuestra cuenta).

Pero 驴Que pasa si necesitamos ejecutar 100 tareas o 1000 tareasd el mismo tipo? 驴Tendremos que escribir 100 o 1000 veces en la consola los nombres de nuestros archivos?

#### 5.1.3 Ejecuci贸n en paralelo por grupo

La diferencia entre la la ejecuci贸n en pararelo individual y en grupo es solo 1, como manejamos la manea de llamar 100 o 1000 tareas al mismo tiempo.

Para esta tarea, podemos usar el mismo archivo standar.sh anterior, por lo que aca no habran cambios, la unica diferencia es la llamada del archivo de ejecuci贸n. Para ello, generaremos un archivo BASH con todas las llamadas de ejecuci贸n que necesitemos realizar y ejecutaremos este archivo autom谩tico, es decir.


```
# Antiguo metodo de ejecuci贸n
$ sbatch standar.sh my_input_file_1.tsv output_one_1.tsv output_two_1.tsv
Submitted batch job 000001
$ sbatch standar.sh my_input_file_2.tsv output_one_2.tsv output_two_2.tsv
Submitted batch job 000002
$ sbatch standar.sh my_input_file_3.tsv output_one_3.tsv output_two_3.tsv
Submitted batch job 000003
$ sbatch standar.sh my_input_file_4.tsv output_one_4.tsv output_two_4.tsv
Submitted batch job 000003
```

Nuevo metodo de ejecuci贸n.

1. Crear un archivo de grupo de ejecuci贸n
```
vim group_1.sh
```
2. Dentro del archivo de grupo de ejecuci贸n, escribiremos todas nuestras tareas. Dado que lo unico que cambia es el nombre de input, podemos utilizar un script simple de AWK, python o EXCEL para poder generar las llamadas.
```
sbatch standar.sh my_input_file_1.tsv output_one_1.tsv output_two_1.tsv
sbatch standar.sh my_input_file_2.tsv output_one_2.tsv output_two_2.tsv
sbatch standar.sh my_input_file_3.tsv output_one_3.tsv output_two_3.tsv
sbatch standar.sh my_input_file_4.tsv output_one_4.tsv output_two_4.tsv
sbatch standar.sh my_input_file_5.tsv output_one_5.tsv output_two_5.tsv
sbatch standar.sh my_input_file_N.tsv output_one_N.tsv output_two_N.tsv
```
3. Finalmente llamaremos al archivo de grupo de ejecuci贸n y automaticamente, todas las llamadas se ejecutaran al mismo tiempo.
```
bash group_1.sh
```
4. Lo que veremos ahora como respuesta en la consola, sera algo como
```
Submitted batch job 000001
Submitted batch job 000002
Submitted batch job 000003
Submitted batch job 000004
Submitted batch job 000005
Submitted batch job 00000N
```

###########################################################################################################
## IMPORTANTE ##
#### 驴Cu谩ntas llamadas podr铆a ejecutar al mismo tiempo? ####
###########################################################################################################

Por lo general, se utilizan llamadas de entre 50 a 100 tareas por cada archivo de ejecuci贸n de grupo, por lo que es necesario separar llamadas de mas de 100 tareas en grupos de este tama帽o. En el caso de tener 500 tareas, generar 5 archivos con grupos de 100 llamadas seria 贸ptimo.
No se recomienda tener mas de 100 tareas en la cola de procesos, por m谩s que tengamos la capacidad de realizar estas 100 tareas al mismo tiempo.
<br>

### 5.2 Monitoreo de tareas

Monitorear tareas es bastante simple, para ello podemos utilizar 2 comandos especificos. 
El primero es:

```
$ squeue
```
Este comando nos dara una mirada r谩pida para revisar las tareas que enviamos a ejecutar y cual es su estado actual.
Si ejecutamos este comando, y acabamos de lanzar nuestras tareas, podremos observar lo siguiente.
```
JOBID    PARTITION            NAME          USER        ST    TIME    NODES    NODELIST(REASON)
000001        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
000002        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
000003        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
000004        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
000005        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
00000N        main    python_script   [your_user] [status]    0:00        1    [REASON:NODE_ID]
```

El segundo comando es:

```
$ watch squeue
```
Este comando nos permitira tener una ventana permanente que se actualizar谩 cada 2 segundos, entregandonos informaci贸n en tiempo real de nuestras tareas.
La informac贸n mostrada, sera exactamente la misma a la anterior.

Revisemos ahora la informaci贸n entregada:

```
JOBID:              Contiene el ID de nuestra tarea, este sera el mismo ID mencionado
                    al ejecutar nuestra tarea.
PARTITION:          Contiene el nombre de la partici贸n donde ejecutamos nuestra tarea.  
NAME:               Nombre de nuestra tarea.
USER:               Nombre de nuestro usuario.
ST:                 Estatus del job que hemos enviado, estos pueden ser:
                    PD: Nuestra tarea esta pendiende de ejecuci贸n (pending)
                    R:  Nuestra tarea se esta ejecutando (running)
                    CG: Nuestra tarea esta terminando (completing)
TIME:               Contiene el tiempo de ejecuci贸n de nuestra tarea.
NODES:              Numero de nodos asignados a nuestra tarea.
NODELIST(REASON):   Nodo en el cual se esta ejecutando nuestra tarea. En en caso de que
                    nuestra tarea se encuentre en el estatus pendiente: (ST : PD ), esta
                    columna desplegar谩 la raz贸n del porque esta pendiente.
                    Resources: Nuestra tarea esta esperando que un nodo tenga los recursos
                    que solicitamos para poder ejecutarse.
                    Priority: Nuestra tarea esta en la cola de procesos y esta esperando
                    a que tareas de otros usuarios terminen
                    QOS[MaxCPULimit]: Nuestra cuenta ya esta utilizando el Maximo numero
                    de CPU asignados a nuestro usuario y tendremos que esperar a que otras
                    de nuestras tareas terminen
```

<br>

### 5.3 Cancelaci贸n de tareas

En el caso de que necesitemos cancelar o terminar forzosamente una tarea (error en el script, error en el uso de recursos, etc), podremos cancerlar solo usando su JOBID mencionado anteriormente, para ello, podremos utilizar el comando:

```
$ scancel [JOBID]

Ejemplo:
$ scancel 000001
```

Con esto, podemos eliminar esta tarea de la cola, y volver a relanzar nuestro script con las modificaciones que necesitemos.



<br>
<br>

## 6. Errores de ejecuci贸n de tareas / jobs. <br>

En un mundo perfecto, todas nuestras tareas correr谩n en el tiempo asignado y finalizar谩n sin ning煤n inconveniente. Mientras eso no exista, revisemos algunos de los errores m谩s frecuentes y c贸mo solucionarlos.

### 6.1 error JOB XXXXXXXXX ON mn00P CANCELLED AT (DATE). ###

Este error es frecuentemente encontrado cuando nuestra tarea est谩 subutilizando recursos y es cancelada autom谩ticamente por el sistema.
Este caso se da si pedimos demasiados CPU o RAM, pero nuestra tarea est谩 utilizando menos del 20% de los recursos solicitados.

Si configuramos correctamente nuestro correo en el archivo de Ejecuci贸n de tareas, podremos recibir un correo del NLHPC (por lo general a la bandeja de spam) con el mensaje:

```
[NLHPC] - Aviso de Subutilizaci贸n de Recursos (MEM):

o

[NLHPC] - Aviso de Subutilizaci贸n de Recursos (CPU):
```

Con este correo, podemos verificar el comportamiento de nuestra tarea y corregir la solicitud de recursos a una m谩s apropiada.
<br>

###########################################################################################################
## IMPORTANTE ##
#### 驴Qu茅 pasa si estoy pidiendo solo 1 CPU y 1 GB de RAM y aun as铆 mi tarea se est谩 cancelando por subutilizaci贸n de recursos? 驴Qu茅 pasa si mi tarea se ejecuta con los recursos correctos durante las primeras horas, pero luego subutiliza recursos? ####

###########################################################################################################

Para este tipo de casos, debemos enviar un correo a soporte indicando nuestro problema. Luego se nos pedir谩 ejecutar la tarea nuevamente y enviar el JOBID para que esta tarea sea excluida de la cancelaci贸n autom谩tica y entre a la "White List" de procesos.


<br>

### 6.2 OOM_KILL EVENT. Some of the step tasks have been OOM Killed. ###

```
check.XXXXXXXXXmn00P.NNNNNNNNN.err:slurmstepd: error: Detected 1 oom_kill event in StepId=XXXXXXXXXX.batch.
Some of the step tasks have been OOM Killed.

o

Detected 1 oom-kill event(s) in StepId=XXXXXXXX.batch.
Some of your processes may have been killed by the cgroup out-of-memory handler.
```

Este ser谩 uno de los principales errores que encontraremos a diario en nuestro archivo #SBATCH -e [ERROR FILE]. Este error indica principalmente que la cantidad de RAM que solicitamos es insuficiente y debemos pedir m谩s en nuestro Archivo de ejecuci贸n.
Existen algunas herramientas que recomiendan un n煤mero definido de RAM, pero para la mayor铆a, seremos nosotros los que decidamos cu谩nta memoria solicitar. **Como buena pr谩ctica, incrementar la RAM en un 20% extra e intentar nuevamente suele ser la mejor opci贸n**

<br>

### 6.3 Segmentation fault (core dumped). Aborted (core dumped) ###

```
/tmp/10781177854440103180/linclust/3581267220862219892/linclust.sh:
line 76: 1624213 Segmentation fault (core dumped) 
```

Este error no es grave, es grav铆simo, al borde de lo radiactivo y letal. Significa que, en el NODO donde estamos ejecutando nuestra tarea, una de las herramientas siendo ejecutadas hizo que el NODO completo colapsara y dejara de correr todas las tareas. Este es un fallo catastr贸fico como el famoso "pantallazo azul" en Windows. Es de vital importancia revisar el archivo de errores y la ejecuci贸n de la tarea para saber si fue nuestra herramienta la que gener贸 el fallo u otra que estaba ejecut谩ndose junto a la nuestra en el mismo NODO. Como se podr谩 asumir, este error termina forzosamente todas las tareas del NODO en que se estaba ejecutando, lo que no solo nos afecta a nosotros, sino al resto de los usuarios.

En el caso de que relancemos nuestra tarea, y este error vuelva a ocurrir, habr谩 que comunicarse con soporte@nlhpc.cl y pedir el uso exclusivo de un NODO, as铆 como el monitoreo extenso de nuestro job. As铆, podemos ver c贸mo solucionar el problema. En el peor de los casos, no podremos seguir ejecutando nuestro programa. En el mejor de los casos, y mayoritariamente por experiencia, bajar el n煤mero de CPU y aumentar la RAM elimina satisfactoriamente este error. El tiempo de ejecuci贸n aumentar谩, pero es mejor terminar m谩s tarde que no terminar. En otras ocasiones, hay herramientas que por defecto, internamente llaman a m谩s de 1 CPU para tareas secundarias, por lo que es esencial conocer los requerimientos internos de cada ejecuci贸n. Este tipo de requerimientos fallidos tambi茅n puede provocar un "core-dumped".

## 7. Agradecimientos al NLHPC en publicaciones

Una vez que hayamos terminado nuestros an谩lisis y podamos finalmente escribir nuestros manuscritos, es imporante agradecer el uso del NLHPC en nuestros textos. Esto debido a que el supercomputador recibe fondos estatales para poder seguir operando y justifica sus gastos operacionales con las contribuciones a la ciencia.
Para ello, en el apartado de **Acknowledgements** de nuestra publicaci贸n debemos escribir:


### "Experiments presented in this paper were carried out using the supercomputing infrastructure of NLHPC at CMM, Universidad de Chile (see https://www.nlhpc.cl/infraestructura/)"

Este sera siempre el paso final al ejecutar nuestras laborales en el NLHPC.




##  Autor

**Ricardo Palma V茅jares** :incoming_envelope: [rpalmavejares@gmail.com](mailto:rpalmavejares@gmail.com)





